{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/haochunniu/Desktop/Python/Advance AI/Final Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement & Project Goal\n",
    "In this project, I try to create different RNN models to classify various twitters in four into sentiments. The main goal of this project is to practice and emplement the important NLP models into real time cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "Read in the twitter dataset and fix the column names. Originally, there are 73996 twitters in the train dataset and four different seintiments (Negative, Positive, Neutral, Irrelevent). The distribution of all four sentiments is quite balance. On the other hand, there are 1000 twitters in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       Source     Label  \\\n",
       "0   0  Borderlands  Positive   \n",
       "1   1  Borderlands  Positive   \n",
       "2   2  Borderlands  Positive   \n",
       "\n",
       "                                                Text  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-1.Import train data\n",
    "train=pd.read_csv('twitter_training.csv',header= None)\n",
    "train.columns=['ID','Source','Label','Text']\n",
    "train['ID']=list(range(len(train)))\n",
    "train=train.dropna()\n",
    "train_text=np.array(train['Text'])\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      30.22\n",
       "Positive      27.91\n",
       "Neutral       24.47\n",
       "Irrelevant    17.40\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(train['Label'].value_counts()/train['Label'].value_counts().sum()*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-2.Create dummy variables for label in train data\n",
    "train_y=pd.get_dummies(train[['Label']],\n",
    "                       prefix='',\n",
    "                       prefix_sep='',\n",
    "                       columns=['Label'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Source       Label  \\\n",
       "0   0   Facebook  Irrelevant   \n",
       "1   1     Amazon     Neutral   \n",
       "2   2  Microsoft    Negative   \n",
       "\n",
       "                                                Text  \n",
       "0  I mentioned on Facebook that I was struggling ...  \n",
       "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       "2  @Microsoft Why do I pay for WORD when it funct...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-3.Import test data\n",
    "test=pd.read_csv('twitter_validation.csv',header=None)\n",
    "test.columns=['ID','Source','Label','Text']\n",
    "test['ID']=list(range(len(test)))\n",
    "test_text=np.array(test['Text'])\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "To train the text classification model, I need to vetorize the original text into word level input. Next, I will use general word embedding to embed the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-1.Text pre-processing for train data\n",
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens = None,\n",
    "    standardize = 'lower_and_strip_punctuation',\n",
    "    split = 'whitespace',\n",
    "    ngrams = None,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42559"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2-2. Apply it to the text data with \"adapt\". After word embedding, there are totally 42,559 unique words.\n",
    "vectorize_layer.adapt(train_text)\n",
    "len(vectorize_layer.get_vocabulary(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model1: Simple RNN\n",
    "In this project, I try 2 different models, simple RNN and RNN with LSTM units. I will start from the simple RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-1. Classification model of simple RNN\n",
    "model_rnn = keras.Sequential()\n",
    "\n",
    "model_rnn.add(vectorize_layer)\n",
    "\n",
    "model_rnn.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 256,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_rnn.add(keras.layers.SimpleRNN(256,return_sequences=True,dropout=0.3)) # see note below\n",
    "model_rnn.add(keras.layers.SimpleRNN(256,dropout=0.3))\n",
    "model_rnn.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "#3-2. Compile the loss function for the model\n",
    "model_rnn.compile(loss = keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "58/58 [==============================] - 84s 1s/step - loss: 0.1139 - accuracy: 0.9559 - val_loss: 2.7176 - val_accuracy: 0.4368\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 90s 2s/step - loss: 0.0971 - accuracy: 0.9615 - val_loss: 2.8951 - val_accuracy: 0.4315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f0c0dc2d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3-3. Add early stopping layer\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=1)\n",
    "\n",
    "#3-4. the postive and negative twitters are way more important that irrelevant and neutral\n",
    "weight = {'Irrelevant': 1.,\n",
    "          'Negative': 10.,\n",
    "          'Neutral': 1.,\n",
    "          'Positive':10.}\n",
    "\n",
    "#3-5. Fit the model\n",
    "model_rnn.fit(x = train_text, y = train_y,\n",
    "              validation_split = 0.2,\n",
    "              epochs=10,\n",
    "              batch_size = 1024,\n",
    "              callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.81      0.87      0.84     12875\n",
      "    Negative       0.91      0.85      0.88     22358\n",
      "     Neutral       0.86      0.83      0.84     18108\n",
      "    Positive       0.85      0.89      0.87     20655\n",
      "\n",
      "    accuracy                           0.86     73996\n",
      "   macro avg       0.86      0.86      0.86     73996\n",
      "weighted avg       0.86      0.86      0.86     73996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3-6. Summary report of prediction on train data\n",
    "train_data_result=pd.DataFrame(model_rnn.predict(train_text), columns = ['Irrelevant','Negative','Neutral','Positive'])\n",
    "train_data_result['max']=train_data_result.max(axis = 1)\n",
    "train_data_result['Label']=np.where(train_data_result['Negative']==train_data_result['max'],'Negative',np.where(train_data_result['Positive']==train_data_result['max'],'Positive',np.where(train_data_result['Neutral']==train_data_result['max'],'Neutral','Irrelevant')))\n",
    "print(classification_report(train['Label'], train_data_result['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.71      0.88      0.79       172\n",
      "    Negative       0.87      0.83      0.85       266\n",
      "     Neutral       0.86      0.76      0.81       285\n",
      "    Positive       0.86      0.87      0.87       277\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.82      0.84      0.83      1000\n",
      "weighted avg       0.84      0.83      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3-7. Summary report of prediction on test data\n",
    "test_data_result=pd.DataFrame(model_rnn.predict(test_text), columns = ['Irrelevant','Negative','Neutral','Positive'])\n",
    "test_data_result['max']=test_data_result.max(axis = 1)\n",
    "test_data_result['Label']=np.where(test_data_result['Negative']==test_data_result['max'],'Negative',np.where(test_data_result['Positive']==test_data_result['max'],'Positive',np.where(test_data_result['Neutral']==test_data_result['max'],'Neutral','Irrelevant')))\n",
    "print(classification_report(test['Label'], test_data_result['Label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model2: RNN with LSTM units\n",
    "Next, I will try a more advance model, which is the RNN model with LSTM units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-1. Classification model of RNN with LSTM units\n",
    "model_lstm = keras.Sequential()\n",
    "\n",
    "model_lstm.add(vectorize_layer)\n",
    "\n",
    "model_lstm.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 512,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_lstm.add(keras.layers.LSTM(128,\n",
    "                                 dropout=0.2,\n",
    "                                 return_sequences=True))\n",
    "\n",
    "model_lstm.add(keras.layers.LSTM(128,\n",
    "                                 dropout=0.2,\n",
    "                                 return_sequences=True))\n",
    "\n",
    "model_lstm.add(keras.layers.LSTM(128,\n",
    "                                 dropout=0.2))\n",
    "\n",
    "model_lstm.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n",
    "#4-2. Compile the loss function for the model\n",
    "model_lstm.compile(loss = keras.losses.categorical_crossentropy,\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "116/116 [==============================] - 201s 2s/step - loss: 0.9184 - accuracy: 0.6290 - val_loss: 1.5728 - val_accuracy: 0.4520\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 201s 2s/step - loss: 0.3827 - accuracy: 0.8624 - val_loss: 1.9585 - val_accuracy: 0.4492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f11b21a90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4-3. Add early stopping layer\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=1)\n",
    "\n",
    "#4-4. the postive and negative twitters are way more important that irrelevant and neutral\n",
    "weight = {'Irrelevant': 1.,\n",
    "          'Negative': 2.,\n",
    "          'Neutral': 1.,\n",
    "          'Positive':2.}\n",
    "\n",
    "#4-5. Fit the model\n",
    "model_lstm.fit(x = train_text, y = train_y,\n",
    "               validation_split = 0.2,\n",
    "               epochs=10,\n",
    "               batch_size = 512,\n",
    "               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.76      0.82      0.79     12875\n",
      "    Negative       0.85      0.84      0.84     22358\n",
      "     Neutral       0.87      0.76      0.81     18108\n",
      "    Positive       0.81      0.86      0.83     20655\n",
      "\n",
      "    accuracy                           0.82     73996\n",
      "   macro avg       0.82      0.82      0.82     73996\n",
      "weighted avg       0.83      0.82      0.82     73996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4-6. Summary report of prediction on train data\n",
    "train_data_result=pd.DataFrame(model_lstm.predict(train_text), columns = ['Irrelevant','Negative','Neutral','Positive'])\n",
    "train_data_result['max']=train_data_result.max(axis = 1)\n",
    "train_data_result['Label']=np.where(train_data_result['Negative']==train_data_result['max'],'Negative',np.where(train_data_result['Positive']==train_data_result['max'],'Positive',np.where(train_data_result['Neutral']==train_data_result['max'],'Neutral','Irrelevant')))\n",
    "print(classification_report(train['Label'], train_data_result['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.80      0.87      0.83       172\n",
      "    Negative       0.86      0.85      0.85       266\n",
      "     Neutral       0.87      0.82      0.85       285\n",
      "    Positive       0.88      0.89      0.89       277\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.86      0.85      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4-7. Summary report of prediction on test data\n",
    "test_data_result=pd.DataFrame(model_lstm.predict(test_text), columns = ['Irrelevant','Negative','Neutral','Positive'])\n",
    "test_data_result['max']=test_data_result.max(axis = 1)\n",
    "test_data_result['Label']=np.where(test_data_result['Negative']==test_data_result['max'],'Negative',np.where(test_data_result['Positive']==test_data_result['max'],'Positive',np.where(test_data_result['Neutral']==test_data_result['max'],'Neutral','Irrelevant')))\n",
    "print(classification_report(test['Label'], test_data_result['Label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model3: RNN with LSTM units and BERT word embedding\n",
    "Originally, I create this one layer LSTM model with BERT word embedding. Yet, I eventually notice that with my computer resource, I am not able to train the model based on this large amount of raw data. Yet, the entire process of creating this model gives me a deeper understanding of the BERT word embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# fetch the pre-trained model (it will download a model file ~500M)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-1.Use BERT to encode texts\n",
    "vectorized_text = tokenizer(train_text.tolist(), return_tensors='tf', padding=True)\n",
    "bert_embeddings = bert_model(vectorized_text)['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-2.Build a single layer LSTM model with BERT embeddings\n",
    "embeddings = keras.layers.Input(shape = (bert_embeddings.shape[1], bert_embeddings.shape[2]))\n",
    "masked_embeddings = tf.keras.layers.Masking(mask_value=0)(embeddings)\n",
    "h_all, h_final, c_final = keras.layers.LSTM(units = 128,return_state = True)(masked_embeddings)\n",
    "pred = keras.layers.Dense(units = 4,activation='softmax')(h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-3.Assemble model\n",
    "model_bert_lstm = keras.Model(inputs = embeddings,outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_bert_lstm.fit(x = bert_embeddings,\n",
    "                    y = train_y,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 10,\n",
    "                    validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
